{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25870b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLAH\n"
     ]
    }
   ],
   "source": [
    "print(\"ALLAH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc53eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: d:\\Medical Chatbot\\Medical-Chatbot-Generative-AI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02421d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: d:\\Medical Chatbot\\Medical-Chatbot-Generative-AI\n"
     ]
    }
   ],
   "source": [
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2b2ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34ea4204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4505 documents\n"
     ]
    }
   ],
   "source": [
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf_file(data='Data/')\n",
    "print(f\"Loaded {len(extracted_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45196fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks: 40000\n"
     ]
    }
   ],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of Text Chunks:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12a9539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Length: 384\n"
     ]
    }
   ],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Embedding Length:\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d3c459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f4616d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index might already exist: (409)\n",
      "Reason: Conflict\n",
      "HTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2025-01', 'x-cloud-trace-context': '6a48ceeb356dd04c31ac4e7dc6eee131', 'date': 'Sun, 25 May 2025 18:25:35 GMT', 'server': 'Google Frontend', 'Content-Length': '85', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"error\":{\"code\":\"ALREADY_EXISTS\",\"message\":\"Resource  already exists\"},\"status\":409}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "# Create index (run only once)\n",
    "try:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384, \n",
    "        metric=\"cosine\", \n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\", \n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "    print(f\"Created index: {index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Index might already exist: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20aa1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = DEEPSEEK_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f31631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekLLM:\n",
    "    def __init__(self, api_key, model=\"deepseek-chat\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.base_url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "    \n",
    "    def invoke(self, prompt):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Handle different prompt formats\n",
    "        if hasattr(prompt, 'format_messages'):\n",
    "            # If it's a ChatPromptTemplate\n",
    "            messages = prompt.format_messages()\n",
    "            formatted_messages = []\n",
    "            for msg in messages:\n",
    "                if hasattr(msg, 'content') and hasattr(msg, 'type'):\n",
    "                    role = \"user\" if msg.type == \"human\" else \"system\" if msg.type == \"system\" else \"assistant\"\n",
    "                    formatted_messages.append({\"role\": role, \"content\": msg.content})\n",
    "        elif isinstance(prompt, str):\n",
    "            # If it's a simple string\n",
    "            formatted_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        else:\n",
    "            # If it's already formatted messages\n",
    "            formatted_messages = prompt\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": formatted_messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.base_url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            return DeepSeekResponse(result['choices'][0]['message']['content'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling DeepSeek API: {e}\")\n",
    "            return DeepSeekResponse(\"I apologize, but I'm unable to process your request at the moment.\")\n",
    "\n",
    "class DeepSeekResponse:\n",
    "    def __init__(self, content):\n",
    "        self.content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a5a4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DeepSeekLLM(api_key=DEEPSEEK_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4094b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing vector store\n",
      "Vector store ready: <langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000002075C8EFB50>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to load existing index first\n",
    "    docsearch = PineconeVectorStore.from_existing_index(\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    print(\"Loaded existing vector store\")\n",
    "except:\n",
    "    # If no existing index, create new one\n",
    "    docsearch = PineconeVectorStore.from_documents(\n",
    "        documents=text_chunks,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings, \n",
    "    )\n",
    "    print(\"Created new vector store\")\n",
    "\n",
    "print(\"Vector store ready:\", docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b69f7411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents for test query\n"
     ]
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for test query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43bdeb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a medical assistant chatbot trained on medical literature. \"\n",
    "    \"Use the following pieces of retrieved context to answer medical questions accurately. \"\n",
    "    \"If you don't know the answer based on the provided context, say that you \"\n",
    "    \"don't have enough information in the medical literature provided. \"\n",
    "    \"Always provide accurate, helpful medical information but remind users to \"\n",
    "    \"consult healthcare professionals for personal medical advice. \"\n",
    "    \"Keep answers clear and informative.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom document chain for DeepSeek\n",
    "def create_deepseek_stuff_documents_chain(llm, prompt):\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    def chain_func(inputs):\n",
    "        context = format_docs(inputs[\"context\"])\n",
    "        formatted_prompt = prompt.format_messages(context=context, input=inputs[\"input\"])\n",
    "        \n",
    "        # Convert to simple format for DeepSeek\n",
    "        messages = []\n",
    "        for msg in formatted_prompt:\n",
    "            role = \"user\" if msg.type == \"human\" else \"system\"\n",
    "            messages.append({\"role\": role, \"content\": msg.content})\n",
    "        \n",
    "        response = llm.invoke(messages)\n",
    "        return response.content\n",
    "    \n",
    "    return chain_func\n",
    "\n",
    "question_answer_chain = create_deepseek_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Custom retrieval chain\n",
    "def create_deepseek_retrieval_chain(retriever, combine_docs_chain):\n",
    "    def chain_func(inputs):\n",
    "        # Retrieve relevant documents\n",
    "        docs = retriever.invoke(inputs[\"input\"])\n",
    "        \n",
    "        # Combine documents and generate answer\n",
    "        answer = combine_docs_chain({\"context\": docs, \"input\": inputs[\"input\"]})\n",
    "        \n",
    "        return {\n",
    "            \"input\": inputs[\"input\"],\n",
    "            \"context\": docs,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "    \n",
    "    return chain_func\n",
    "\n",
    "rag_chain = create_deepseek_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "866d9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Medical Chatbot Test ===\n",
      "\n",
      "Question: What is Acromegaly and gigantism?\n",
      "Error calling DeepSeek API: 402 Client Error: Payment Required for url: https://api.deepseek.com/v1/chat/completions\n",
      "Answer: I apologize, but I'm unable to process your request at the moment.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What are the symptoms of diabetes?\n",
      "Error calling DeepSeek API: 402 Client Error: Payment Required for url: https://api.deepseek.com/v1/chat/completions\n",
      "Answer: I apologize, but I'm unable to process your request at the moment.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: How is hypertension treated?\n",
      "Error calling DeepSeek API: 402 Client Error: Payment Required for url: https://api.deepseek.com/v1/chat/completions\n",
      "Answer: I apologize, but I'm unable to process your request at the moment.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What causes acne?\n",
      "Error calling DeepSeek API: 402 Client Error: Payment Required for url: https://api.deepseek.com/v1/chat/completions\n",
      "Answer: I apologize, but I'm unable to process your request at the moment.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def ask_medical_question(question):\n",
    "    \"\"\"Function to ask questions to the medical chatbot\"\"\"\n",
    "    try:\n",
    "        response = rag_chain({\"input\": question})\n",
    "        return response[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error processing question: {e}\"\n",
    "\n",
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What is Acromegaly and gigantism?\",\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is hypertension treated?\",\n",
    "    \"What causes acne?\"\n",
    "]\n",
    "\n",
    "print(\"=== Medical Chatbot Test ===\")\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer = ask_medical_question(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b21a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_chatbot():\n",
    "    \"\"\"Interactive medical chatbot\"\"\"\n",
    "    print(\"=== Medical Chatbot ===\")\n",
    "    print(\"Ask me any medical question based on the loaded medical literature.\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \")\n",
    "        if question.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"Thank you for using the Medical Chatbot!\")\n",
    "            break\n",
    "        \n",
    "        if question.strip():\n",
    "            answer = ask_medical_question(question)\n",
    "            print(f\"\\nAnswer: {answer}\")\n",
    "            print(\"\\nRemember: This information is for educational purposes only. Always consult with healthcare professionals for personal medical advice.\")\n",
    "\n",
    "# Uncomment the line below to start interactive mode\n",
    "# medical_chatbot()\n",
    "\n",
    "print(\"\\n=== Setup Complete ===\")\n",
    "print(\"Your medical chatbot is ready!\")\n",
    "print(\"Use ask_medical_question('your question') to get answers\")\n",
    "print(\"Or run medical_chatbot() for interactive mode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
